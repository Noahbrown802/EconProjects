{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 3 ###\n",
    "Hey Nelson! Back with a classification job about Costa Rican Poverty Levels. My game plan is to start with SUPER DUPER simple and nieve models and then build them to be better and better! Some of my worries are about the imbalanced data, the multiple classification groups, and the amount of features. Hopefully combating these will be easy to complete with the resources I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#confusion Matricies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Visualizing Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells are just to get a good idea of what our data looks like. On the back end it may or may not include the \"data description.txt\" file haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>v2a1</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>...</th>\n",
       "      <th>SQBescolari</th>\n",
       "      <th>SQBage</th>\n",
       "      <th>SQBhogar_total</th>\n",
       "      <th>SQBedjefe</th>\n",
       "      <th>SQBhogar_nin</th>\n",
       "      <th>SQBovercrowding</th>\n",
       "      <th>SQBdependency</th>\n",
       "      <th>SQBmeaned</th>\n",
       "      <th>agesq</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_279628684</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>1849</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1849</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_f29eb3ddd</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>4489</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4489</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_68de51c94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>8464</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>8464</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_d671db89c</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>289</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>289</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_d56d6f5f5</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>1369</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id      v2a1  hacdor  rooms  hacapo  v14a  refrig  v18q  v18q1  \\\n",
       "0  ID_279628684  190000.0       0      3       0     1       1     0    NaN   \n",
       "1  ID_f29eb3ddd  135000.0       0      4       0     1       1     1    1.0   \n",
       "2  ID_68de51c94       NaN       0      8       0     1       1     0    NaN   \n",
       "3  ID_d671db89c  180000.0       0      5       0     1       1     1    1.0   \n",
       "4  ID_d56d6f5f5  180000.0       0      5       0     1       1     1    1.0   \n",
       "\n",
       "   r4h1   ...    SQBescolari  SQBage  SQBhogar_total  SQBedjefe  SQBhogar_nin  \\\n",
       "0     0   ...            100    1849               1        100             0   \n",
       "1     0   ...            144    4489               1        144             0   \n",
       "2     0   ...            121    8464               1          0             0   \n",
       "3     0   ...             81     289              16        121             4   \n",
       "4     0   ...            121    1369              16        121             4   \n",
       "\n",
       "   SQBovercrowding  SQBdependency  SQBmeaned  agesq  Target  \n",
       "0         1.000000            0.0      100.0   1849       4  \n",
       "1         1.000000           64.0      144.0   4489       4  \n",
       "2         0.250000           64.0      121.0   8464       4  \n",
       "3         1.777778            1.0      121.0    289       4  \n",
       "4         1.777778            1.0      121.0   1369       4  \n",
       "\n",
       "[5 rows x 143 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/Noah/Desktop/ECON 213/CostaRica/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9557, 143)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay first things firss. I've heard that the data is imbalanced as far as our target variable goes, and I want to get a good visualization of that. I'm going to get a count of how many of each we have, and then of course a graph that also has shows the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    9557.000000\n",
      "mean        3.302292\n",
      "std         1.009565\n",
      "min         1.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         4.000000\n",
      "max         4.000000\n",
      "Name: Target, dtype: float64\n",
      "4    5996\n",
      "2    1597\n",
      "3    1209\n",
      "1     755\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.Target.describe())\n",
    "print(data.Target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1198e0ccac8>,\n",
       "  <matplotlib.axis.XTick at 0x1198e0cc400>,\n",
       "  <matplotlib.axis.XTick at 0x1198e0cc2e8>,\n",
       "  <matplotlib.axis.XTick at 0x1198e19ca90>],\n",
       " <a list of 4 Text xticklabel objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAFeCAYAAACSOvhTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XeYnGW9xvHvTe+EEpAWuijqETQURSlSRZEiePCoBIwCioqAHkFRFET0KCBFRBAERERAEVQUEQQrvReRiAgxlGAA6S33+eN5xgxhs5lJdvedzd6f69orM8+8885vJ7Pze58u20RERHRqrqYDiIiI4SWJIyIiupLEERERXUniiIiIriRxREREV5I4IiKiK0kc0RFJJ0r6/ACda4ykJyTNXe9fLulDA3Huer5fSho3UOfr4nW/LOlhSQ8M9Wv3CkmbSprYdBwxuJI4Akn3SHpa0uOSHpX0J0l7S/rP58P23rYP6/BcW/R3jO17bS9i+8UBiP2Lks6c7vxvt3367J67yzhWAg4A1rb9ij4e31TS1JowH5d0p6Q9hjLGGsfukv4wm+dYX9JF9bMyRdLVTfwu0ZwkjmjZzvaiwMrAV4HPAKcM9ItImmegz9kjVgb+Zfuhfo6ZZHsRYDHK+3uypLWHJDoG5r2X9CbgMuAKYA1gKeAjwNtn99wxjNjOzwj/Ae4BtpiubH1gKvDaev804Mv19tLAz4FHgSnA7ykXId+vz3kaeAL4X2AVwMB44F7gd21l89TzXQ4cAVwNPAZcACxZH9sUmNhXvMA2wHPA8/X1bmo734fq7bmAg4F/AA8BZwCL18dacYyrsT0MfK6f92nx+vzJ9XwH1/NvUX/nqTWO0/p4bl+/x2Rg53r7XcBt9T29HHh1LT8QOG+65x0DHNsW0ynA/cA/gS8Dc9fHdgf+CBxd/59+DDwDvFjjfBRYD3iw9X9Rn/du4MYZvAd/AL7Vz3v0kt+zxv834HHgdmDHtsfWoCSgx+p7/6NarhrzQ/Wxm5n2OZwf+Eb9/3oQOBFYsL/PZdN/X3PiT2oc0SfbVwMTgbf28fAB9bHRwLLAZ8tT/AHKH/R2Lk1R/9f2nE2AVwNbz+AldwM+CCwPvAAc20GMvwK+QvnCWcT26/s4bPf6sxmwGrAIcPx0x7wFWAvYHPiCpFfP4CWPo3xRr1Z/n92APWz/hnLFPanGsXt/cUuaS9KOwCjgFkmvBH4IfJLynl4E/EzSfLV8W0mL1efODbwHOKue7nTK+7UGsC6wFdDeX7QBcDewDPB+YG/gzzXOUbavAf4FbNn2nPdTLgKmj3sh4E3Aef39ftP5G+UztDjwJeBMScvVxw4Dfg0sAaxIeX+pv8PGwCvre/TfNUaAr9XydervvALwhfpYn5/LLmKNDiVxRH8mAUv2Uf48sBywsu3nbf/e9ZKvH1+0/aTtp2fw+Pdt32r7SeDzwHtaneez6X3AUbbvtv0EcBCw63TNNl+y/bTtm4CbgJcloBrLfwMH2X7c9j3AkcAHuohleUmPUq6uDwE+YPvOet5f2L7E9vOUK+oFgTfb/gdwPbBDPcfbgKdsXylpWUrC+mR9bx+iXKnv2vaak2wfZ/uFft770ynJAklLUpL7WX0ctwTlO+P+Tn9h2+fanmR7qu0fAXdRarNQPkcrA8vbfsb2H9rKFwVeBcj2HbbvlyTgw8B+tqfYfpxy4bBr2/O6/VzGLEjiiP6sQKnyT+/rwATg15LulnRgB+e6r4vH/wHMS2l6mF3L1/O1n3seyhVpS/soqKcotZLpLQ3M18e5Vugilkn1Kn9J2+vYPruvGG1PpbwfrXOfBby33v4fpn2pr0x5n+6vHdWPAt+h1C5aZva+A5wJbCdpEUpt5ve2+0oOj1Ca45br47E+SdpN0o1t8b2Waf+v/0tplrpa0m2SPghg+zJKrfBbwIOSTqo1rtHAQsB1bef7VS2HWftcxixI4og+SVqP8sX1shE49Yr7ANurAdsB+0vavPXwDE45syu/ldpuj6FcPT4MPEn5smjFNTfTvig6Oe8kyhds+7lfoLSPd+Nhpl0ht5/rn12epy8vibFeWa/Udu5zgU0lrQjsyLTEcR/wLLB0TUijbC9m+zVt557+/XnZ+2X7n8Cf67k/QB/NVPW4p+px7+7kl5K0MnAy8DFgKdujgFspyQLbD9j+sO3lgb2AEyStUR871vYbgddQmqY+Tfk/eBp4Tdvvu7jLgIOZfS5jACVxxEtIWkzSO4GzgTNt39LHMe+UtEb9gvs3pbO1NbT2QUofQLfeL2nt2o5+KKVD+EXgr8ACkt4haV5Kh/T8bc97EFilfejwdH4I7Cdp1XpF3eoTeaGb4Gos5wCHS1q0finuT7lan13nAO+QtHn9HQ+gJIQ/1deeTOkw/x7wd9t31PL7KX0ER9b/t7kkrS5pk35e60Fgxdp/0u4MSg3gdcD5/Tz/f4HdJX1a0lIAkl4v6ew+jl2Ykqgm1+P2oNQ4qPd3qckQSm3GwIuS1pO0QX0vnqR26Nea2MnA0ZKWqedYQdLW9XZ/n8sYQEkc0fIzSY9TrmI/BxwFzGhs/prAbygjc/4MnGD78vrYEcDBtSnhU128/vcpI7ceABYAPgFg+zHgo8B3KVfgT1I6QFvOrf/+S9L1fZz31Hru3wF/p3wJfbyLuNp9vL7+3ZSa2Fn1/LOl9nO8n9I5/DDlank728+1HXYWZfTW9H0Pu1Ga0G6nfPmeR/9NSZdRRm89IOnhtvLzKbWe82s/04xi/ROln+VtwN2SpgAnUTr0pz/2dko/0J8pCet1lFFeLesBV0l6ArgQ2Nf23ynDlU+uv88/KB3j36jP+QylOepKSf+mfA7Xqo/197mMAaT0HUUEgKS/AXvVUWIRM5QaR0Qg6d2UpqLLmo4let+cOos3Ijok6XJgbcrw4KkNhxPDQJqqIiKiK2mqioiIriRxREREV+bIPo6ll17aq6yyStNhREQMK9ddd93DtkfP7Lg5MnGsssoqXHvttU2HERExrEj6x8yPSlNVRER0KYkjIiK6ksQRERFdSeKIiIiuDGrikDRK0nmS/iLpDklvkrSkpEsk3VX/XaIeK0nHSpog6WZJb2g7z7h6/F2Sxg1mzBER0b/BrnEcA/zK9qsou6rdQdmD+FLbawKX1vtQdjJbs/7sCXwb/rMj2SGULTDXBw5pJZuIiBh6g5Y46o5dGwOnANh+zvajwPaUrSqp/7a2xNweOMPFlcCoujfx1sAldavIR4BLgG0GK+6IiOjfYNY4VqNs4PI9STdI+q6khYFlW9tS1n9b21yuwEu3uZxYy2ZUHhERDRjMCYDzAG8APm77KknHMK1Zqi/qo8z9lL/0ydKelCYuxowZ0320ETFHumLj/jZEHFk2+d0VA3KewaxxTAQm2r6q3j+PkkgerE1Q1H8faju+fd/pFSl7Mc+o/CVsn2R7rO2xo0fPdMZ8RETMokFLHLYfAO6T1NrWcXPK9pYXAq2RUeOAC+rtC4Hd6uiqDYHHalPWxcBWkpaoneJb1bKIiGjAYK9V9XHgB5Lmo+zTvAclWZ0jaTxwL7BLPfYiYFvKfsJP1WOxPUXSYcA19bhDbU8Z5LgjImIGBjVx2L4RGNvHQ5v3cayBfWZwnlOBUwc2uoiImBWZOR4REV1J4oiIiK4kcURERFeSOCIioitJHBER0ZUkjoiI6EoSR0REdCWJIyIiupLEERERXUniiIiIriRxREREV5I4IiKiK0kcERHRlSSOiIjoShJHRER0JYkjIiK6ksQRERFdSeKIiIiuJHFERERXkjgiIqIrSRwREdGVJI6IiOhKEkdERHQliSMiIrqSxBEREV1J4oiIiK4MauKQdI+kWyTdKOnaWrakpEsk3VX/XaKWS9KxkiZIulnSG9rOM64ef5ekcYMZc0RE9G8oahyb2V7H9th6/0DgUttrApfW+wBvB9asP3sC34aSaIBDgA2A9YFDWskmIiKGXhNNVdsDp9fbpwM7tJWf4eJKYJSk5YCtgUtsT7H9CHAJsM1QBx0REcVgJw4Dv5Z0naQ9a9mytu8HqP8uU8tXAO5re+7EWjaj8oiIaMA8g3z+jWxPkrQMcImkv/RzrPoocz/lL31ySUx7AowZM2ZWYo2IiA4Mao3D9qT670PA+ZQ+igdrExT134fq4ROBldqeviIwqZ/y6V/rJNtjbY8dPXr0QP8qERFRDVrikLSwpEVbt4GtgFuBC4HWyKhxwAX19oXAbnV01YbAY7Up62JgK0lL1E7xrWpZREQ0YDCbqpYFzpfUep2zbP9K0jXAOZLGA/cCu9TjLwK2BSYATwF7ANieIukw4Jp63KG2pwxi3BER0Y9BSxy27wZe30f5v4DN+yg3sM8MznUqcOpAxxgREd3LzPGIiOhKEkdERHQliSMiIrqSxBEREV1J4oiIiK4kcURERFeSOCIioitJHBER0ZUkjoiI6EoSR0REdCWJIyIiupLEERERXUniiIiIriRxREREV5I4IiKiK0kcERHRlZkmDkkLSfq8pJPr/TUlvXPwQ4uIiF7USY3je8CzwJvq/YnAlwctooiI6GmdJI7Vbf8f8DyA7acBDWpUERHRszpJHM9JWhAwgKTVKTWQiIgYgebp4JhDgF8BK0n6AbARsPtgBhUREb1rponD9iWSrgc2pDRR7Wv74UGPLCIietIME4ekN0xXdH/9d4ykMbavH7ywIiKiV/VX4ziyn8cMvG2AY4mIiGFghonD9mZDGUhERAwPnUwAXEDS/pJ+IunHkj4paYFOX0DS3JJukPTzen9VSVdJukvSjyTNV8vnr/cn1MdXaTvHQbX8Tklbd/9rRkTEQOlkOO4ZwGuA44DjgbWB73fxGvsCd7Td/xpwtO01gUeA8bV8PPCI7TWAo+txSFob2LXGsA1wgqS5u3j9iIgYQJ0kjrVsj7f92/qzJ/DKTk4uaUXgHcB3631R+kbOq4ecDuxQb29f71Mf37wevz1wtu1nbf8dmACs38nrR0TEwOskcdwgacPWHUkbAH/s8PzfBP4XmFrvLwU8avuFen8isEK9vQJwH0B9/LF6/H/K+3hOREQMsU4SxwbAnyTdI+ke4M/AJpJukXTzjJ5UF0J8yPZ17cV9HOqZPNbfc9pfb09J10q6dvLkyTMKKyIiZlMnM8e3mcVzbwS8S9K2wALAYpQayChJ89RaxYrApHr8RGAlYKKkeYDFgSlt5S3tz/kP2ycBJwGMHTv2ZYklIiIGxkxrHLb/AYwCtqs/o2z/o/XTz/MOsr2i7VUonduX2X4f8Ftg53rYOOCCevvCep/6+GW2Xct3raOuVgXWBK7u8veMiIgB0slw3H2BHwDL1J8zJX18Nl7zM8D+kiZQ+jBOqeWnAEvV8v2BAwFs3wacA9xOWTNrH9svzsbrR0TEbOikqWo8sIHtJwEkfY3Sz3Fcpy9i+3Lg8nr7bvoYFWX7GWCXGTz/cODwTl8vIiIGTyed4wLar/BfJPtxRESMWJ3UOL4HXCXp/Hp/B6Y1L0VExAjTybLqR0m6HHgLpaaxh+0bBjuwiIjoTZ00VQEsBDxu+xjKcNlVBzGmiIjoYZ2MqjqEMhLqoFo0L3DmYAYVERG9q5Max47Au4AnAWxPAhYdzKAiIqJ3dZI4nqsT8QwgaeHBDSkiInpZJ4njHEnfoSwV8mHgN8DJgxtWRET0qk5GVX1D0pbAv4G1gC/YvmTQI4uIiJ7Ub+KQtAOwBnCL7U8PTUgREdHLZthUJekEYD/KelKHSfr8kEUVERE9q78ax8bA622/KGkh4PfAYUMTVkRE9Kr+Osefa61Ca/spsj5VRETQf43jVW07/AlYvd4XYNv/NejRRUREz+kvcbx6yKKIiIhhY4aJo7/d/SIiYuTqdJHDiIgIIIkjIiK61N88jkvrv18bunAiIqLX9dc5vpykTYB3STqb6Ybj2r5+UCOLiIie1F/i+AJwILAicNR0jxl422AFFRERvau/UVXnAedJ+rztzBiPiAigs9VxD5P0LsoSJACX2/754IYVERG9qpOtY48A9gVurz/71rKIiBiBZlrjAN4BrGN7KoCk04EbmLYHeUREjCCdzuMY1XZ78cEIJCIihodOEscRwA2STqu1jeuAr8zsSZIWkHS1pJsk3SbpS7V8VUlXSbpL0o8kzVfL56/3J9THV2k710G1/E5JW8/KLxoREQNjponD9g+BDYGf1J832T67g3M/C7zN9uuBdYBtJG0IfA042vaawCPA+Hr8eOAR22sAR9fjkLQ2sCvwGmAb4ARJc3f+K0ZExEDqqKnK9v22L7R9ge0HOnyObT9R785bf1rzP86r5acDO9Tb29f71Mc3l6RafrbtZ23/HZgArN9JDBERMfAGda0qSXNLuhF4CLgE+BvwqO0X6iETgRXq7RWA+wDq449Rtq39T3kfz4mIiCE2qInD9ou216HMPl+fvvf4cP23rx0G3U/5S0jaU9K1kq6dPHnyrIYcEREz0W/ikDSXpFtn90VsPwpcTukrGSWpNQx4RWBSvT0RWKm+7jyU0VtT2sv7eE77a5xke6ztsaNHj57dkCMiYgb6TRx17sZNksZ0e2JJoyWNqrcXBLYA7gB+C+xcDxsHXFBvX1jvUx+/zLZr+a511NWqwJrA1d3GExERA6OTCYDLAbdJuhp4slVo+10dPO/0OgJqLuAc2z+XdDtwtqQvUyYSnlKPPwX4vqQJlJrGrvV1bpN0DmXW+gvAPrZf7Pg3jIiIAdVJ4vjSrJzY9s3Aun2U300fo6JsPwPsMoNzHQ4cPitxRETEwOpkkcMrJK0MrGn7N5IWAjKPIiJihOpkkcMPU+ZVfKcWrQD8dDCDioiI3tXJcNx9gI2AfwPYvgtYZjCDioiI3tVJ4njW9nOtO3Wo7MvmUURExMjQSeK4QtJngQUlbQmcC/xscMOKiIhe1UniOBCYDNwC7AVcBBw8mEFFRETv6mRU1dS6nPpVlCaqO+vEvIiIGIFmmjgkvQM4kbJAoYBVJe1l+5eDHVxERPSeTiYAHglsZnsCgKTVgV8ASRwRESNQJ30cD7WSRnU3ZZn0iIgYgWZY45C0U715m6SLgHMofRy7ANcMQWwREdGD+muq2q7t9oPAJvX2ZGCJQYsoIiJ62gwTh+09hjKQiIgYHjoZVbUq8HFglfbjO1hWPSIi5kCdjKr6KWWvjJ8BUwc3nIiI6HWdJI5nbB876JFERMSw0EniOEbSIcCvgWdbhbavH7SoIiKiZ3WSOF4HfAB4G9OaqlzvR0TECNNJ4tgRWK19afWIiBi5Opk5fhMwarADiYiI4aGTGseywF8kXcNL+zgyHDciYgTqJHEcMuhRRETEsNHJfhxXDEUgERExPHQyc/xxpu0xPh8wL/Ck7cUGM7CIiOhNndQ4Fm2/L2kHYP1BiygiInpaJ6OqXsL2T8kcjoiIEWumiUPSTm0/O0v6KtOarvp73kqSfivpDkm3Sdq3li8p6RJJd9V/l6jlknSspAmSbpb0hrZzjavH3yVp3Gz8vhERMZs6GVXVvi/HC8A9wPYdPO8F4ADb10taFLhO0iXA7sCltr8q6UDgQOAzwNuBNevPBsC3gQ0kLUkZ2TWWkrCuk3Sh7Uc6iCEiIgZYJ30cs7Qvh+37gfvr7ccl3QGsQEk6m9bDTgcupySO7YEzbBu4UtIoScvVYy+xPQWgJp9tgB/OSlwRETF7+ts69gv9PM+2D+v0RSStAqwLXAUsW5MKtu+XtEw9bAXgvranTaxlMyqf/jX2BPYEGDNmTKehRUREl/rr43iyjx+A8ZQaQkckLQL8GPik7X/3d2gfZe6n/KUF9km2x9oeO3r06E7Di4iILvW3deyRrdu1j2JfYA/gbODIGT2vnaR5KUnjB7Z/UosflLRcrW0sBzxUyycCK7U9fUVgUi3fdLryyzt5/YiIGHj9jqqqI6C+DNxMSTJvsP0Z2w/197z6XFF2DrzD9lFtD10ItEZGjQMuaCvfrY6u2hB4rDZpXQxsJWmJOgJrq1oWEREN6K+P4+vATsBJwOtsP9HluTei7ONxi6Qba9lnga8C50gaD9wL7FIfuwjYFpgAPEWp3WB7iqTDgGvqcYe2OsojImLo9Teq6gDKargHA58rFQig9Dl4ZkuO2P4DffdPAGzex/EG9pnBuU4FTu3v9SIiYmj018fR9azyiIiY8yU5REREV5I4IiKiK0kcERHRlSSOiIjoShJHRER0JYkjIiK6ksQRERFd6WQ/joh+3Xvo65oOoWeM+cItTYcQMehS44iIiK4kcURERFeSOCIioitJHBER0ZUkjoiI6EoSR0REdCWJIyIiupLEERERXUniiIiIriRxREREV5I4IiKiK0kcERHRlSSOiIjoSlbHjegxGx23UdMh9Iw/fvyPTYcQfUiNIyIiupLEERERXRm0xCHpVEkPSbq1rWxJSZdIuqv+u0Qtl6RjJU2QdLOkN7Q9Z1w9/i5J4wYr3oiI6Mxg1jhOA7aZruxA4FLbawKX1vsAbwfWrD97At+GkmiAQ4ANgPWBQ1rJJiIimjFoicP274Ap0xVvD5xeb58O7NBWfoaLK4FRkpYDtgYusT3F9iPAJbw8GUVExBAa6j6OZW3fD1D/XaaWrwDc13bcxFo2o/KIiGhIr3SOq48y91P+8hNIe0q6VtK1kydPHtDgIiJimqFOHA/WJijqvw/V8onASm3HrQhM6qf8ZWyfZHus7bGjR48e8MAjIqIY6sRxIdAaGTUOuKCtfLc6umpD4LHalHUxsJWkJWqn+Fa1LCIiGjJoM8cl/RDYFFha0kTK6KivAudIGg/cC+xSD78I2BaYADwF7AFge4qkw4Br6nGH2p6+wz0iIobQoCUO2++dwUOb93GsgX1mcJ5TgVMHMLSIiJgNvdI5HhERw0QSR0REdCWJIyIiujJil1V/46fPaDqEnnHd13drOoSIGEZS44iIiK4kcURERFeSOCIioitJHBER0ZUkjoiI6EoSR0REdCWJIyIiupLEERERXUniiIiIriRxREREV5I4IiKiK0kcERHRlSSOiIjoShJHRER0JYkjIiK6ksQRERFdSeKIiIiuJHFERERXkjgiIqIrSRwREdGVJI6IiOhKEkdERHRl2CQOSdtIulPSBEkHNh1PRMRINSwSh6S5gW8BbwfWBt4rae1mo4qIGJmGReIA1gcm2L7b9nPA2cD2DccUETEiyXbTMcyUpJ2BbWx/qN7/ALCB7Y+1HbMnsGe9uxZw55AH2r2lgYebDmIOkvdzYOX9HDjD5b1c2fbomR00z1BEMgDUR9lLMp7tk4CThiacgSHpWttjm45jTpH3c2Dl/Rw4c9p7OVyaqiYCK7XdXxGY1FAsEREj2nBJHNcAa0paVdJ8wK7AhQ3HFBExIg2LpirbL0j6GHAxMDdwqu3bGg5rIAyrprVhIO/nwMr7OXDmqPdyWHSOR0RE7xguTVUREdEjkjgiIqIrSRwRlaS5prvf1zDwiBEviWOYan2pSVqq6VjmFLanAkg6TNI8Tgdg9JBeupBJ4him2r7U9pL0DuitD9ZwpGIhYF1g97ayvK99yPsytHrpQiaJYxiTNC8wBfiEpP/qpQ/WcOTiKeBLwHaStqxleV/btDXpLShpZUnrSVqm0aDmQK33WdImkj4h6dPTN6c2pSeCiFlj+3nbJwIXAZ+StCK8vK0++tfW7LcYgO1rgG8Cn5W0QX1s7uYi7DmtRPp/wMHAV4D/gdRCBoqkuWxPlbQGcALwN+BrwJubjazIF8wwJWkzSeMlLQj8hrIsyy4wra0+OmPbkl4J/FXS0ZLOBNYD7gcOrn/ELzYbZe+o79cmwOuBvShLAP2xPrxuXd0hBsZBwKHAo8BFtv8g6XWSPtjkxUwSxzAkaWnglcDOlCXmdwfeBxwi6bOS5s2V38xJmkfSqHp3LWBL4KfA8cAylBWWlwZ+WPs+YpplgNMoy/9cZfsaSatRrooXbDKwOUHbxd/VwAPAEcBhtWwXYN0mL2aGxZIj8Z/mp+VtT6T8cf7M9tslrQ48AfwI+C/gbcDatm9qLtphYy1gfUlvBl5r+01tj11Z//2SpJOBdYA/DXWAvUTS62zfUu9eB3yb8r5sUcsOBK61/VgT8c0JJL0C+DRwru0rgduB04EHbF8laTNgB2DbBsPMkiPDhaQVKG3KY4D5ba8/g+O+BKxp+3+GMr7hSNKywHjgk8C5wPeAv9r+t6TFgXltPyzpj8Chti9uMNxGSRoDXA/8EjjE9t2StqHUOOYH5qXUQt5m+4XmIh3eJK1DqVEsSenX+A7lfT0DeAG4B/iT7e80FSMkcQwrkhYBfkFppvoq8L36JbcMsJ7tX0jaD3jG9rebjLXXSVJtq98feANwM7ACcDdlMc3DgeNsXy5pA9tXNRhuT6jNdV8F3km5Cj6K0r+xNLAwcIPtB5uLcM4gaXnKrqdvBRYHfmr755KWsf1Qs9EVSRzDQNsIi/dTPkiXAsdQrkS+BIwDbrN9cINhDhttSWNF4DTbW9RBBjtS5nAsC4yxvWmTcfaK1ii9tgmSrQuXVYGv2D63wfDmGJLmbu+3qP1vOwIbAk8Bv7V9Yevz21SckMTR89qSxnzAl4Hvt9qZJb2H0lRwp+2DalnjH6rhQtJuwObA+FbzSm2SeQF4yvajkua1/XyTcTat9YVWR1LNDzwCXAtsBnwDeBbY0vYTDYY5rLX9na9JGfSyGaVWdy5lE7v3As/a/nqDYf5HEscwIenrwAeAd9v+43SPzW/72daHr5kIh5c6Mu2blA7yM4BfAX/L+/dSrcRZ+zMOpwxRvoeyL85htidJ2t72BU3GOZzVi8LFan/aL4CbgNuAj1GS8nhgMmUk9OPNRTpNEkePa2tWeQVwHPAO4FO2T2g4tGGvDlnenNLUN5EyF+Fa2w80GliPqAMyvg2cSOmwPd72dZJeC/w3sJDtA5qMcU4g6QDKxL4fAVvY3rPtsa8A89j+36bi60vmcfS4mjSWBB63vQvli25vSX+RtGXD4Q0rrQlTknaogwguAP5OmWT1KPBRYI3mIuw5LwKXUJLE+sBqALZvBY4GNqyjgGL2HE/ZHvujwHqSxklatD52BvA2SfM3Fl0fUuPoUW0MKBBlAAAWqklEQVTtytsAHwSeAZ60/ZH6+CeBVW3v22Scw0VbzW0RSs3iQ5Q25ONaI9AkvdL2X5uMs9fUQQNrAntS5gidC5wMLAKcbnuDBsMb9tqblyWtRJmJvxFl2PPjwKuB+3qlb6MliaPHSbqJsg7QFymd4AdP/wWXvo3OSfocpd34IuAk22+pa1TtDxxl+9+NBtgj+vpMSdqYskrBLsCtwGdt/7aB8OYIbR3iCwKLAi/YniLpjZT+jU2Bk21/pck4+5KZ4z1M0uuB3wETKBP/xteHDpZ0ou0/Qdam6tJvKetQHUlpogLYCXhjkkbRVtt9NWWW8qaUJPtj4HeS/gAsnqQx21pX7YcBywMLSLoVOM/2HpLeDfyzsej6kT6OHtNaY6o2rdxESe53AT+pk/22BF7ZShrRuXoldxXwdkqzyzOSNgI+TpkPk1Vwgba5BEdSJkTeA5wr6XZJ69s+lTJQI2ZRW9Pp+sAmwIcpi0YuCxwq6SDgirrsSM9J4ugxbXMwPiRpU+C7lIXOlpV0IvB5ylpV+ZLrkIoxlFn377S9DWVdpdMozS6n2b62/jFnFVxA0taUZUTOo3yhLQP8GbhS0u5ZVmT2tP2dv52yNP22lLkxe1NqIv9FD38/p4+jB6ls0PRRYDnbB9Ymg7WA1SlXIdc2GuAwJektlBFC/2f7vj4ez+TJqnbUzktZjuV9tnesNbadgCMy2W/WtTUFrgqMpszZ2B+YYPuHkr4M3G/7W40G2o+ezWgjWZ2pfC6wlqSLgEdt/9T2ka2k0WrSihnro0Z2FWW+xqWS3jr98SM9aWjajnP7ATvavpvSCf6IpHdRaroTkjRm3XS12tOAB20/SVkr7QxJZwDvro/1rNQ4esR06ydNtT2plu8HPGz7+81GODzV9X5+BJxJWX7+CmA7yvo/BzpLgAMvG658FqWW8XidQ/RJyiKGS9reo9FAh7m2kVTvpaxifWhb2WsoAxGusX11s5H2L4mjx0g6DNie0t75e8peB2Mpw/K+0WRsw03bH+THgOWABShNLbdR2pQ3sf37JmPsNfULbT/KXI3TbT9Xyxdy2Y89ZkNtKZiXst/L64FdbP+k2ai6l8TRA9rb1mu7578pC53NR5mtuzPwRdunNBfl8NHWhjwKWILSV/Sn+tgCwAbAi7b/0GScvaiO2tua0vZ+JfBH2zc3G9WcSdI4yirXVwB7DaelbpI4GtZ2Vbwk5Wp4FPAg8Bvb99djRtueXG+nA7dDks6nzL5dnTJO/lN1LkL7MXk/q5pUl7F9r8rKy+sBAm4BzsyIs1nXdjGzBqWmMR9wGfAwZbHNfYA9bJ/eYJgdS+LoEZLOBp6k1DYeoVzxHW/7zkYDG2ba2up3oVzFbVHLdwA+C+xj+5pGg+whbV9oewBvoayFdqnt8Sp7iO8E/H36hBuzRtKvKMPr3wmc1Wp+Vt0C2sNkI6yMqmpQ22S/VwGr2x5vez9K5+SClNEV0YW22sM8lK1OkTSf7Z8C51M6xaOqSWMuygS0zwI/pSzhDWWQxjeSNAaGpP+m7B3+Bcq+JmfV8p0pkyx7Yne/TiRxNKjtS+5u4DlJO9XyCZRZuxtKWrip+IYbSa+sw0ahrOq6haT9Wx28lBVen2kmup72ZsrSNktRFtj7Qi0/ts59iYHxGHCnpG8DP3DZy2Qz4DOUr4Nh0/yTpqqGSPowZUP6pSnLKq9LqWHcSdnzek/KUur7px1+5iTtTRlEsAJl5u0elGa/MymbDl0LLG9728aC7FG1xnE8sDHwSdu/kbQdZbjyRs1GN+eQtDhlb5OxwG62/yzp15QkMiz6NlqSOBqgslT6VyhfastQFjD8NKXT7M2UdZR+Tdlh7cUkjv6p7KD2Z2BX23dJ+gTwDttb18c3Be4FprhsB/uSvZ0D6oTIkykLal4PbAkcZPvyJuMazvTSbZ+Xsn1/nauxG/AqYDHgDtsfbTTQWZDE0QBJlwMHuOymNgb4OnCs65awdcaz64cuS6bPhKSPU/YseY/tu2rZ74Bxtv/eaHA9qK1DfD1gq1p8qe0rJX0IeAq42z26wN5wI+l44I2Uvew/Tdk87GnKniaTPQz3tM+y6kOsXv1uTNmg5bo69HFpynr8TH81nKTRkbuA3wAflnQdpZ3+Dtt/r0l4amps07R9vr5JGWp7D7BXHXl2hu3bm4ptTtFW29iU0gz9Vsp6VCdS+pNOHM7vc2ocDZC0FXAoZfjtjZSd/HZKk9SsU9kfextKc9+WlK1NT7f9bKOB9Zi2L7TNgDfbPlzSUpSmk7dQLmqOtH1Zo4HOISQdSpls2lq2fyngCMpAjTcP19n4SRxDaPrEUJfC2IvS/v4+2482Ftww1cd7uhZlqerlgeeBi23/rqn4eoleuk3pX4HFKcuu/KWWrURZPylJYzZIWs323bUZ+muU1R9OoDQHTqzHvGI4zRSfXobjDqG2ZUXmrvePp8zOvRu4VdIXm4tueOrjPb3T9jcpW8MuQGlLjuIASevU26+jzNm4TtJXAGzfl6Qxe+qFyxmSvgBMoVwYnkIZSfVBSdtLWng4Jw1IjaMxdQjkXK4b4qhsE/tq22c3G9nwVd9T2q6qF3ZZsnpEqxNNF6Fc9S5D6Q861fa/6uTTE4A3ATvYvri5SIc/SYtSLgZ3AFalDLU9W2VPnZ2BNYEv2/5rg2HOtiSOIdBf30X9o1Z7J3j6OjonaXnKH+NE23+rZS97T6NQ2ap0f8oy6T9oXajUtan+avvGJuMbzqZrCpybkih2Ap4DjrN9taR1bd/QZJwDIYljCLR1SG5LaXt/Griy9UUX3WkbTroNsC9wB2XUypbpJ3q5ts/fGMoIs4l1La8PA/dR1ky6tNko5xySvg6cWxPFssCulGHPfwP29xyw7W76OAZZ2x/tGyn7hS8LfJWyFhWSMiS6S23DSQ+hrK/0DGX570clbSBplaZi60VtNa8PAQdL2hH4GfAuSv/awVnaZkA9DZwp6WTgOdvHUP72L58TkgakxjFkJH0XOJ2yHManbW9fV8R8NfCLNE11rjZFvQI4APgSpc1+O9sPSTod+LHtC5uMsZfopXtc7wC8BrgfuMD2tZIWs/3vZqOcs9Qm1EMoO/odb/u4ZiMaWKlxDIFaq7idsoHQcZQvPIBPUZpXkjQ60Or8BnDZq2RJSlPLL2vS2AJ4bZJGIelVKntszCVpedt/t300ZRLaGsDpksYnaQwMSRtKWk3SPLYn2d4LOBs4XGVnxTlGmkmGgO0XJF1MmZR2BzC3pLGUq5E3wUs71mKGlgEeoKza+hPgI5S9S7auwyCXpjQDvmwG/gi1O2Wb3EnAoZJ+AHzH9rXAe+tnMp3hs6H2W75g+9fAxygXM6dJusz2w5RZ4o/a/mGTcQ60NFUNkrbmgeWBV9i+vs4Y3xFYCXgCuND2WfmSmzlJywEfpTRRvRl4q+0pkhYBVqYs63CF7fsaDLNn1NrZe4EDKYtp3gTsQulbO5uyUdhWtv+7sSDnAJL2AT4BfND2HyXtBnyAsozLDZT92z9h+5fNRTnwkjgGQWs4raRXAD+hrEP1JOVDdCtlRvNcw3W5gabUNvqfUrbdPAT4cytRzCnDHAeapLcDe1OaSO8E3kFZEPIByuq3tzUY3hxBZYuEdYGP14vFhSkT/0zZGuG7jQY4CJI4BpGkA4H5bB9ar0z2Ba4BjrB9a7PRDR+1M5yajNcDFgI+SVlt9NuUmsi/bX+wuSh7U51PsC9luPI+tifV8iVsP9JocMNc7ct4oQ65PYnSlPqhkZCM0zk+wFpLX6hs2nIV8CiA7W8Br6UMHT1ghieIl2jV3tqSBsCNtncEfgGMp2x1ul89fkR/piWtKuldqjsh2n7R9lHAlcAPa18QSRqzR9KomjTmsv2g7e0pe+hs23aMmotwcKXGMUgkXUZJzItQxsz/zHZrD+x52j506RDvR9s8mIMoK4rOR0m+lwKn2X4q7+c0NWH8D6V5dC3KZ28B4GrKvI1/2v5YcxEOf5JeCfwc+BNwHWWQ0ZmU/ew/B1wAfHVOHi2ZxDGA9NINcvYG9gE2o3zhLQf8EziW0u45or/guqGyX8nltl8r6eeU93EZShvyGbZ/2miAPabtc7gBsDpleZH3UL7gRtlet9EAhzlJoylbFG9Hmey3EWUF3J9RBiSsCmxg+5rGghxkSRwDrFZPL6I0pxxUy1ajjAR6FfDFOWX26FCRtBNlCfAbKZOpNqrj4ncD9nNdFjz6p7IXxLO2n2g6ljlFW434lZShuAsAb7P9hYZDG1SZxzGAav/G3JSheLtKeoGyEubdkv5BudpLk0oH2v4gV6fUMC4B3kBZ7wfK+3x3ksbMtd5L2/9qOpY5hdpWt5Y0v1+62u3lDYU1ZEZ0R+JAmW5G83OUUT4bU77o/lRn577Y+sNN0pi5tvfoYMo8mMcp25yuJekvlBVevwXpEJ+ZfN4GR1vLwY8lvRbm7A7xdqlxDIC2P8zDJU2lbJJzhO13SNoO+Kaka2zf3FyUw09djG8eSscutqcAG0jaEHjC9u2pvUUTPG359PcAC7aG18/JHeLtcqU2m1pXGHVW+BspE9TWAJ6ua1RdZHv1JI1ZsjzwPuD7kuZrFdq+su0PNUkjBl3bMPu1JL2nbWj4C5Ta73+OGQmSOGZT2xXGJpTlHVYBrq6zmFcDvtb+pRf9a6/q17kvC1PWWnqoDsmNGFJ1LtGLkhakDHzZAvh1XfvrN7Zvgpcs9z/HS+KYDa229frv7ykb0x9BWW4Ayl4Rz9t+bqS0fc6O2uxkSctK+nxNFDvb3o2yIORukn7VbJQxgr0P+IntPSlrpr0I3CPpmzBy+jcgiWOW1auQqfUq5FjKSIqrgb8Dn5H0Ucr6NZ9rLsrhpa3Z6XhKE8D6wJb1sRttv5qygNyIahaIZrWtO7cxsJCk0bafrRc0m1FqxSOmfwOSOGZH6+ri3ZQJfc9Q1qv5JuVqZB5gXE0uc4+kD9XskPRqYBHbRwBLUd5TJH1U0tq2J8PIahaInrAwZWmbVYFdJL1R0sK2b7L94YZjG3KZADgb6ozm6yk7+H2klilJYtap7Iv9GWAU8LTtD6ksTX85sJntfzYZX4wcbTPwW6tdC1ibsu8GwLWUv/0HmouyGalxzJ6FgIMoGwn9XGVp7ySNLkk6RtKuALbvpTT5vRa4XdLalH6jc2z/M3M2Yqi01WpPlnQaZUuEN9eLxCsoTVePNxReo1Lj6JKk19i+TdK6wCdtj5O0JGU3up0oO67tTblazpvbAUkfpAxpvB74OuUP9IOU/dg3AH5FmRczNTW6GApttY39gLdQlqZfgbqvie0PSFrc9mONBtqQTADs3msk/Z6yGVNrH+HHbR8u6TzKMt/zO5s0daSOpDpV0nOU2tsNwDHA4bZPkTQvZWtOZ7JfDJWaNOalzM060/ZEYKKktwLflrSG7QnNRtmcVPu7ZPscykipR4HvSXqv7efrw5sAR9l+JE0qM9c2Mu01wJ7A1sDrKR2Qt0v6lO3nWzWMJI0YCpLeBFD/rn8J/Gd7XdvPUvo5Vm4mut6QL7cOtc0QX5iy29xawMeBIyT9UdJewEdaHWX5kpu56SZP3m97ou3bbO9E2d/gc5LWbC7CGGlqjWJvSfvVC5rzgCclPSjpcEknAg/avrTZSJuVxNGhti+5HSn7NmP7QturAJdR5hzsC5ljMAvOp+Tm9dvKJlCWTL9rJE2sisbdBVxM2e9lb+BdtsdTasOmzBz/aHPh9YZ0jnegvW1d0iKUDtxjsqT3wJH0AUofxyXAs8AOwEa2J6dDPIaC6k6S9faSlM/g6vXh84EbMn+oSOLogqTNKSvfbgGsCXybsgvYP4ADbT/UYHjDRtuIlZ0pS89vAhwC3EF5PydTmgP+0Dq2wXBjBJC0DmXHztspS4nMA9wM7AxMpXxGzwD+LxcxGVU1U5LGUj5AqwBfpGwPeQ1lDPc8wJHAk7YfypXxzLUtGLc4cChwAGUI89cpG2DtYfvR1vFJGjFE1qWMiJxc/90aWAJYGpifslJEhthXqXH0o46u2N/2LvV+e5PVJ4Cl3bZFZBJH5+pggk1tv7et7DvAJbbPay6yGKnqxcx3Kcv5j2sNt5W0hO1HGg2ux6RzvH+HUCafIWljYOO2jtpLgZ0lvao19DZJo391OZGWS4GpklZtK/srsPnQRhVR2H6sXiR+DLhY0nmSlkzSeLkkjhmQNB54pE5Cm4syY/SptuTwF2Bf23/J0NuZk7QscIWkb0pap17N3QP8UNLetQbyPuDEenw+m9EIl7101qAMxX1A0v80HFLPSVPVDEi6DZhq+3V1ifR1XNbhbz3+deBo25PSRNW/Wkv7L+DnwN8o6/tcRenXGAu8nzLAYKLtM/N+Rq+QND+wqO2Hm46ll6RzfMZeBxwn6QlgQWDF1gOSPgWsaHsSpIlqZur7c5OkPYDtgV9QlnL4MfDd1srCEb2mzhR/tuk4ek2aA2bA9lTb+wCvoSw7cLWkVkfuDsAXIE0qXfoD8E/gYOA6SkfkuyX9ur2vI4k4orelqapDkt4BHEWZv3G07QOy6N7MSdoWeCuldgGlSWodYE3bJ0haBdjA9o+aiTAiupXE0aXa3PID133Ec3XcP0l3UpLtb4DvUzrAF6csmb677Z9Ov2FOg+FGRAeSOGZRahudqR3jn6UsJ3IaZWHItSjb606y/dfmoouIWZHEEUOi9mF8jzID/zNpmooYvpI4YkhJ2g74BmXb3TcD/0zNLWJ4SeKIRtQlW060/VzTsUREd5I4olHpK4oYfpI4IiKiK5m8FhERXUniiIiIriRxREREV5I4YkSR9KKkGyXdKulcSQsN8ut9dhae88RgxFLPvbuk4wfr/DEyJHHESPO07XVsvxZ4Dth7MF5ExVyUWfMRc5QkjhjJfk/ZsAdJ+9dayK2SPlnLvlb3YqHe/6KkA+rtT0u6RtLNkr5Uy1aRdIekE4DrgVOABWsN5weSDpO0b9v5Dq/zWWZK0mhJP66veY2kjSTNJekeSaPajpsgadm+jp/9tyuisp2f/IyYH+CJ+u88wAXARyh7g9wCLAwsAtwGrFt/rmh77u3AGGAr4CRAlIuvnwMbU5ZTmQpsOP3r1durANfX23NRNrVaakYxTld2FvCWensMcEe9fQywR729AfCbmRy/O3B80/8P+RneP9nIKUaaBSXdWG//nlIr+Ahwvu0nAST9BHir7WMlLSNpeWA0ZSvhe2stYSvghnqeRSgrAN8L/MP2lX29sO17JP1L0rrAssANtv/VYdxbAGtP2/KexSQtCvyIsjfM94Bd6/3+jo+YbUkcMdI8bXud9gK1fbv24TxgZ8pqvme3ngIcYfs7051nFeDJmbz+dylX/a8ATu00aEoN5U22n57uNf8MrCFpNGWDsS/P5PguXjKib+njiIDfATtIWkjSwsCOlNoIlGSxKyV5nFfLLgY+KGkRAEkrSFpmBud+XtK8bffPB7YB1qvn6dSvgY+17khaB/6zW+L5lE3G7mirwfR5fMRASI0jRjzb10s6Dbi6Fn3X9g31sdtqE88/bd9fy34t6dXAn+sV/BPA+4EX+zj9ScDNkq63/T6XDcB+Czxqu6/jARaSNLHt/lHAJ4BvSbqZ8nf7O6aNCPsRcA2lJtPS3/ERsyVrVUUMoTpE93pgF9t3NR1PxKxIU1XEEJG0NjABuDRJI4az1DgiIqIrqXFERERXkjgiIqIrSRwREdGVJI6IiOhKEkdERHQliSMiIrry/15MBzqSAslEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data.Target)\n",
    "plt.title('Distribution of Poverty Classes')\n",
    "plt.xlabel('Poverty Level')\n",
    "plt.ylabel('Number of People')\n",
    "plt.xticks([0, 1, 2, 3],\n",
    "           ['Extreme Poverty', 'Moderate Poverty', 'Vulnerable', 'Non-Vulnerable'], rotation = 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pretty good idea of what our target variable looks like, Let's check out the Null data. It seems that this is a very important step to take care of near the beginning of a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rez_esc            7928\n",
       "v18q1              7342\n",
       "v2a1               6860\n",
       "meaneduc              5\n",
       "SQBmeaned             5\n",
       "techozinc             0\n",
       "techoentrepiso        0\n",
       "techocane             0\n",
       "techootro             0\n",
       "cielorazo             0\n",
       "abastaguadentro       0\n",
       "sanitario3            0\n",
       "abastaguafuera        0\n",
       "abastaguano           0\n",
       "public                0\n",
       "planpri               0\n",
       "noelec                0\n",
       "coopele               0\n",
       "sanitario1            0\n",
       "sanitario2            0\n",
       "Target                0\n",
       "sanitario5            0\n",
       "elimbasu4             0\n",
       "etecho1               0\n",
       "epared3               0\n",
       "epared2               0\n",
       "epared1               0\n",
       "elimbasu6             0\n",
       "elimbasu5             0\n",
       "elimbasu3             0\n",
       "                   ... \n",
       "parentesco1           0\n",
       "estadocivil7          0\n",
       "estadocivil6          0\n",
       "estadocivil5          0\n",
       "estadocivil3          0\n",
       "instlevel1            0\n",
       "estadocivil2          0\n",
       "estadocivil1          0\n",
       "female                0\n",
       "male                  0\n",
       "dis                   0\n",
       "eviv3                 0\n",
       "parentesco4           0\n",
       "parentesco5           0\n",
       "parentesco6           0\n",
       "parentesco7           0\n",
       "parentesco8           0\n",
       "parentesco9           0\n",
       "parentesco10          0\n",
       "parentesco11          0\n",
       "parentesco12          0\n",
       "idhogar               0\n",
       "hogar_nin             0\n",
       "hogar_adul            0\n",
       "hogar_mayor           0\n",
       "hogar_total           0\n",
       "dependency            0\n",
       "edjefe                0\n",
       "edjefa                0\n",
       "Id                    0\n",
       "Length: 143, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Only 5 columns with missing data. Although these parts of the data may portray important parts of the story, for my super niave model I'm just going to drop the 5 features. I feel comfortable doing this because I have 143 features as it is. 5 less isn't that big of a deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['rez_esc','v18q1','v2a1','meaneduc','SQBmeaned'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target             0\n",
       "planpri            0\n",
       "sanitario5         0\n",
       "sanitario3         0\n",
       "sanitario2         0\n",
       "sanitario1         0\n",
       "coopele            0\n",
       "noelec             0\n",
       "public             0\n",
       "techozinc          0\n",
       "abastaguano        0\n",
       "abastaguafuera     0\n",
       "abastaguadentro    0\n",
       "cielorazo          0\n",
       "techootro          0\n",
       "techocane          0\n",
       "sanitario6         0\n",
       "energcocinar1      0\n",
       "energcocinar2      0\n",
       "energcocinar3      0\n",
       "energcocinar4      0\n",
       "elimbasu1          0\n",
       "elimbasu2          0\n",
       "elimbasu3          0\n",
       "elimbasu4          0\n",
       "elimbasu5          0\n",
       "elimbasu6          0\n",
       "epared1            0\n",
       "epared2            0\n",
       "epared3            0\n",
       "                  ..\n",
       "female             0\n",
       "estadocivil1       0\n",
       "estadocivil2       0\n",
       "estadocivil3       0\n",
       "estadocivil4       0\n",
       "estadocivil5       0\n",
       "estadocivil6       0\n",
       "estadocivil7       0\n",
       "parentesco1        0\n",
       "parentesco2        0\n",
       "parentesco3        0\n",
       "parentesco4        0\n",
       "parentesco5        0\n",
       "parentesco7        0\n",
       "instlevel2         0\n",
       "parentesco8        0\n",
       "parentesco9        0\n",
       "parentesco10       0\n",
       "parentesco11       0\n",
       "parentesco12       0\n",
       "idhogar            0\n",
       "hogar_nin          0\n",
       "hogar_adul         0\n",
       "hogar_mayor        0\n",
       "hogar_total        0\n",
       "dependency         0\n",
       "edjefe             0\n",
       "edjefa             0\n",
       "instlevel1         0\n",
       "Id                 0\n",
       "Length: 138, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again let's dumb down our features a bit just to keep things running quickly and so we can get a good idea of what we may have. I'm only going to grab numerical data first because I think it is super easy to work with. Also because we just will have numerical data we can run some correlations to see what things are most correlated with our target variable. With the house pricing lab it was enough to just use the 10 most correlated things to predict housing prices to a fairly accurate price level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target         1.000000\n",
      "cielorazo      0.304421\n",
      "escolari       0.302305\n",
      "SQBescolari    0.296577\n",
      "eviv3          0.294222\n",
      "epared3        0.292451\n",
      "pisomoscer     0.280284\n",
      "paredblolad    0.261274\n",
      "etecho3        0.257378\n",
      "SQBedjefe      0.246368\n",
      "v18q           0.238864\n",
      "rooms          0.226208\n",
      "instlevel8     0.214351\n",
      "Name: Target, dtype: float64\n",
      "\n",
      "elimbasu5               NaN\n",
      "hogar_nin         -0.328199\n",
      "r4t1              -0.316745\n",
      "SQBhogar_nin      -0.311186\n",
      "overcrowding      -0.289110\n",
      "SQBovercrowding   -0.258744\n",
      "r4m1              -0.253163\n",
      "r4h1              -0.229889\n",
      "eviv1             -0.208038\n",
      "pisocemento       -0.205439\n",
      "epared1           -0.203025\n",
      "hacdor            -0.191714\n",
      "Name: Target, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9557, 133)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndata = data.select_dtypes(include=[np.number])\n",
    "corr = ndata.corr()\n",
    "print(corr['Target'].sort_values(ascending=False)[:13])\n",
    "print('')\n",
    "print(corr['Target'].sort_values(ascending=False)[:-13:-1])\n",
    "ndata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have printed the most positively correlated and the most negatively correlated features for which to build my first model off of. The highest is about .3, and the lowest is -.32. I'm hoping that between these 23 features I can get an f1 higher than .70. From there I should be able to make a model that rocks that up to about .85. *crosses fingers* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cielorazo</th>\n",
       "      <th>escolari</th>\n",
       "      <th>SQBescolari</th>\n",
       "      <th>eviv3</th>\n",
       "      <th>epared3</th>\n",
       "      <th>pisomoscer</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>etecho3</th>\n",
       "      <th>SQBedjefe</th>\n",
       "      <th>v18q</th>\n",
       "      <th>...</th>\n",
       "      <th>r4t1</th>\n",
       "      <th>SQBhogar_nin</th>\n",
       "      <th>overcrowding</th>\n",
       "      <th>SQBovercrowding</th>\n",
       "      <th>r4m1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>eviv1</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>epared1</th>\n",
       "      <th>hacdor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cielorazo  escolari  SQBescolari  eviv3  epared3  pisomoscer  paredblolad  \\\n",
       "0          1        10          100      0        0           1            1   \n",
       "1          1        12          144      0        0           0            0   \n",
       "2          1        11          121      1        0           1            0   \n",
       "3          1         9           81      1        1           1            1   \n",
       "4          1        11          121      1        1           1            1   \n",
       "\n",
       "   etecho3  SQBedjefe  v18q   ...    r4t1  SQBhogar_nin  overcrowding  \\\n",
       "0        0        100     0   ...       0             0      1.000000   \n",
       "1        0        144     1   ...       0             0      1.000000   \n",
       "2        1          0     0   ...       0             0      0.500000   \n",
       "3        1        121     1   ...       1             4      1.333333   \n",
       "4        1        121     1   ...       1             4      1.333333   \n",
       "\n",
       "   SQBovercrowding  r4m1  r4h1  eviv1  pisocemento  epared1  hacdor  \n",
       "0         1.000000     0     0      1            0        0       0  \n",
       "1         1.000000     0     0      0            0        0       0  \n",
       "2         0.250000     0     0      0            0        0       0  \n",
       "3         1.777778     1     0      0            0        0       0  \n",
       "4         1.777778     1     0      0            0        0       0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data[['cielorazo','escolari','SQBescolari','eviv3','epared3','pisomoscer','paredblolad','etecho3','SQBedjefe','v18q','rooms','instlevel8','hogar_nin','r4t1','SQBhogar_nin','overcrowding','SQBovercrowding','r4m1','r4h1','eviv1','pisocemento','epared1','hacdor']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Tyler did his example in class I REALLY liked how he worked through one hyper-parameter at a time and just tuned his models that way. It made a lot of sense to me and with only 23 features I'm hoping that each fit will take less than a minute. I'm going to start with a Random Forest because THEY MAKE SO MUCH SENSE TO ME. I understand how to tune them depending on what my f1 scores are. This will be very helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Target\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I want to check is whether or not I need to have a balanced class weight or no weights at all. When Tyler did this it drastically was changing his scores because he had imbalanced classes as well. I, however, will start here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:   11.4s remaining:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   17.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': ['balanced', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'class_weight':['balanced',None]}\n",
    "#max_depth = [1, 3, 5, 10]\n",
    "#class_weights = ['balanced', None]\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs = -1, max_depth = 3, n_estimators = 1000)\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.5401632, 4.2722315]),\n",
       " 'std_fit_time': array([0.48070293, 0.16427516]),\n",
       " 'mean_score_time': array([0.80063788, 0.84469414]),\n",
       " 'std_score_time': array([0.10280517, 0.14962726]),\n",
       " 'param_class_weight': masked_array(data=['balanced', None],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'class_weight': 'balanced'}, {'class_weight': None}],\n",
       " 'split0_test_score': array([0.57128194, 0.53506994]),\n",
       " 'split1_test_score': array([0.56849657, 0.54084713]),\n",
       " 'split2_test_score': array([0.55380452, 0.54060545]),\n",
       " 'mean_test_score': array([0.56453084, 0.53883962]),\n",
       " 'std_test_score': array([0.00766651, 0.0026687 ]),\n",
       " 'rank_test_score': array([1, 2]),\n",
       " 'split0_train_score': array([0.59798637, 0.5414148 ]),\n",
       " 'split1_train_score': array([0.57093527, 0.5372101 ]),\n",
       " 'split2_train_score': array([0.56630316, 0.54001358]),\n",
       " 'mean_train_score': array([0.57840827, 0.53954616]),\n",
       " 'std_train_score': array([0.01397237, 0.00174809])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. I was expecting a bigger gap between my two scores. Neither parameter seem to be over-fitting an extra ordinary amount, so I'll just go with balanced because it does a smidge better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to check the size of my forest. I'm going from a very small forest to a very large forest. I'm worried that the size 100 won't be enough to predict well, but that the 2500 will over fit too hard. Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:   15.5s remaining:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   36.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=3, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [100, 500, 1000, 2500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'n_estimators':[100, 500, 1000, 2500]}\n",
    "#max_depth = [1, 3, 5, 10]\n",
    "#class_weights = ['balanced', None]\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs = -1, max_depth = 3, class_weight = 'balanced')\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train, y_train)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.3772521 ,  1.79095292,  4.45635398, 12.70527093]),\n",
       " 'std_fit_time': array([0.02797391, 0.05893381, 0.63631277, 0.04684218]),\n",
       " 'mean_score_time': array([0.66566865, 0.45675031, 0.97592918, 1.80752571]),\n",
       " 'std_score_time': array([0.63277541, 0.04891316, 0.12994891, 0.05288604]),\n",
       " 'param_n_estimators': masked_array(data=[100, 500, 1000, 2500],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_estimators': 100},\n",
       "  {'n_estimators': 500},\n",
       "  {'n_estimators': 1000},\n",
       "  {'n_estimators': 2500}],\n",
       " 'split0_test_score': array([0.56350834, 0.56890255, 0.57188222, 0.56900261]),\n",
       " 'split1_test_score': array([0.54973353, 0.56744401, 0.56975469, 0.56859955]),\n",
       " 'split2_test_score': array([0.55068155, 0.5579611 , 0.55631024, 0.56058342]),\n",
       " 'mean_test_score': array([0.55464398, 0.56477119, 0.56598519, 0.56606334]),\n",
       " 'std_test_score': array([0.00628305, 0.00485027, 0.00689349, 0.00387687]),\n",
       " 'rank_test_score': array([4, 3, 2, 1]),\n",
       " 'split0_train_score': array([0.59298202, 0.59277755, 0.59682709, 0.59399884]),\n",
       " 'split1_train_score': array([0.55678799, 0.56928881, 0.56939426, 0.56939157]),\n",
       " 'split2_train_score': array([0.56210452, 0.56738457, 0.56714989, 0.56829076]),\n",
       " 'mean_train_score': array([0.57062484, 0.57648364, 0.57779041, 0.57722706]),\n",
       " 'std_train_score': array([0.01595721, 0.01154773, 0.01349211, 0.01186795])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisngly they all preformed at basically the same level! Wow. I'll probably just keep it at 500 because I don't think that will take too much computing power, but will still be enough that if our model get's a little psychedelic it should still predict well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's check the depth of each tree. I have 1 in there to see if stumping it will be good, and 10 splits could also be reasonable because I have 23 different features. They're needs to be quite a few splits to differentiate all of those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:   10.1s remaining:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [1, 3, 5, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1_weighted',\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'max_depth':[1,3,5,10]}\n",
    "#max_depth = [1, 3, 5, 10]\n",
    "#class_weights = ['balanced', None]\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs = -1,  n_estimators = 500, class_weight = 'balanced')\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train, y_train)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.7358199 , 1.86474975, 2.94718806, 3.2636466 ]),\n",
       " 'std_fit_time': array([0.03983789, 0.04909927, 0.87667389, 0.34646745]),\n",
       " 'mean_score_time': array([0.45822533, 0.51992234, 0.78270205, 0.8365473 ]),\n",
       " 'std_score_time': array([0.04910835, 0.00159718, 0.22360893, 0.20483824]),\n",
       " 'param_max_depth': masked_array(data=[1, 3, 5, 10],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 1},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 10}],\n",
       " 'split0_test_score': array([0.52741116, 0.57408035, 0.60053331, 0.73869543]),\n",
       " 'split1_test_score': array([0.51923012, 0.56309027, 0.60547587, 0.7426388 ]),\n",
       " 'split2_test_score': array([0.51702373, 0.561596  , 0.58823682, 0.72495492]),\n",
       " 'mean_test_score': array([0.52122384, 0.5662582 , 0.59808393, 0.73543194]),\n",
       " 'std_test_score': array([0.00446893, 0.00556732, 0.00724688, 0.00757847]),\n",
       " 'rank_test_score': array([4, 3, 2, 1]),\n",
       " 'split0_train_score': array([0.55061744, 0.59601949, 0.64362304, 0.85624469]),\n",
       " 'split1_train_score': array([0.51634251, 0.56469906, 0.62330683, 0.83896821]),\n",
       " 'split2_train_score': array([0.52546999, 0.56997041, 0.62062459, 0.85311195]),\n",
       " 'mean_train_score': array([0.53080998, 0.57689632, 0.62918482, 0.84944162]),\n",
       " 'std_train_score': array([0.0144932 , 0.0136923 , 0.01026792, 0.00751543])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a very difficult choice to make now. 1-5 are awful models. .6 for my f1 just isn't acceptable. Buuut am I willing to increase that by .13 when I KNOW I'm overfitting significantly? I'm going to try a few deeper trees just to check out if I can bump up training and testing scores while keeping a similar level of overfitting that I can work on later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:   18.1s remaining:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   38.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [10, 15, 20, 50]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1_weighted',\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'max_depth':[10,15,20,50]}\n",
    "#max_depth = [1, 3, 5, 10]\n",
    "#class_weights = ['balanced', None]\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs = -1,  n_estimators = 1000, class_weight = 'balanced')\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train, y_train)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 5.14751132,  6.45698404,  9.45510729, 10.40600332]),\n",
       " 'std_fit_time': array([0.88846846, 0.22353188, 2.05668979, 1.06113751]),\n",
       " 'mean_score_time': array([1.25814247, 1.13346982, 1.75302076, 2.2511843 ]),\n",
       " 'std_score_time': array([0.01143972, 0.0544212 , 0.31641829, 0.36896282]),\n",
       " 'param_max_depth': masked_array(data=[10, 15, 20, 50],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 10},\n",
       "  {'max_depth': 15},\n",
       "  {'max_depth': 20},\n",
       "  {'max_depth': 50}],\n",
       " 'split0_test_score': array([0.75134977, 0.8310581 , 0.82818947, 0.82725516]),\n",
       " 'split1_test_score': array([0.74687592, 0.83932767, 0.83519373, 0.83815148]),\n",
       " 'split2_test_score': array([0.72694951, 0.82206699, 0.81770482, 0.8192459 ]),\n",
       " 'mean_test_score': array([0.74172952, 0.83081879, 0.82703086, 0.82821843]),\n",
       " 'std_test_score': array([0.01060544, 0.00704752, 0.00718555, 0.00774694]),\n",
       " 'rank_test_score': array([4, 1, 3, 2]),\n",
       " 'split0_train_score': array([0.86362847, 0.9799866 , 0.98707792, 0.98651276]),\n",
       " 'split1_train_score': array([0.84291262, 0.98274157, 0.98903887, 0.98847133]),\n",
       " 'split2_train_score': array([0.84930059, 0.98094601, 0.98884161, 0.98846887]),\n",
       " 'mean_train_score': array([0.85194723, 0.98122473, 0.98831946, 0.98781766]),\n",
       " 'std_train_score': array([0.0086618 , 0.00114185, 0.00088159, 0.0009227 ])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a max depth of 15 is going to potentially give me the best shot at getting a good score. I'm overfitting by a lot, but as I make my model more simple that should helpe a bunch! I'm going to get rid of all the features that are negatively correlated and just use the 10 or 12 that were most positively correlated. I'm also going to try 10 and 15 for my max depths. I want to try less again because I have less features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = data[['cielorazo','escolari','SQBescolari','eviv3','epared3','pisomoscer','paredblolad','etecho3','SQBedjefe','v18q','rooms','instlevel8']]\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(train2, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    7.6s remaining:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   13.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   13.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [7, 10, 15]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1_weighted',\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'max_depth':[7,10,15]}\n",
    "clf = RandomForestClassifier(n_jobs = -1,  n_estimators = 1000, class_weight = 'balanced')\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.88016208, 2.5603207 , 3.20715984]),\n",
       " 'std_fit_time': array([0.26557026, 0.18181746, 0.21728922]),\n",
       " 'mean_score_time': array([0.48584255, 0.64644607, 0.59561261]),\n",
       " 'std_score_time': array([0.05675522, 0.05207761, 0.19613057]),\n",
       " 'param_max_depth': masked_array(data=[7, 10, 15],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 7}, {'max_depth': 10}, {'max_depth': 15}],\n",
       " 'split0_test_score': array([0.58462848, 0.61879107, 0.62566576]),\n",
       " 'split1_test_score': array([0.56944427, 0.62574786, 0.64330796]),\n",
       " 'split2_test_score': array([0.55961758, 0.61126144, 0.6219894 ]),\n",
       " 'mean_test_score': array([0.57123514, 0.61860113, 0.63032091]),\n",
       " 'std_test_score': array([0.0102892 , 0.00591461, 0.00930419]),\n",
       " 'rank_test_score': array([3, 2, 1]),\n",
       " 'split0_train_score': array([0.62769788, 0.74238923, 0.86560503]),\n",
       " 'split1_train_score': array([0.61581762, 0.73863736, 0.86175655]),\n",
       " 'split2_train_score': array([0.62095552, 0.74262136, 0.86317527]),\n",
       " 'mean_train_score': array([0.62149034, 0.74121598, 0.86351229]),\n",
       " 'std_train_score': array([0.00486482, 0.00182582, 0.0015891 ])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is just horrible. I don't quite understand though. I thought if you were over fitting that simplifying a model was the way to cut into that over fitting. Yikes. Well, let's try the opposite! I'm going to throw in ALL 133 features and see what our puppy pops out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\pandas\\core\\frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "ndata.drop(axis = 1, columns = ['Target'], inplace = True) \n",
    "X_trainn, X_testn, y_trainn, y_testn = train_test_split(ndata, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:   11.1s remaining:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   24.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   24.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=15, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [10, 15, 30]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1_weighted',\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'max_depth':[10,15,30]}\n",
    "clf = RandomForestClassifier(n_jobs = -1,  n_estimators = 1000, max_depth = 15, class_weight = 'balanced')\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_trainn, y_trainn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([2.59160813, 4.33207758, 9.13894661]),\n",
       " 'std_fit_time': array([0.55381489, 0.38937587, 0.51133914]),\n",
       " 'mean_score_time': array([0.5119652 , 1.04634507, 0.53114748]),\n",
       " 'std_score_time': array([0.08104208, 0.08163421, 0.09937777]),\n",
       " 'param_max_depth': masked_array(data=[10, 15, 30],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 10}, {'max_depth': 15}, {'max_depth': 30}],\n",
       " 'split0_test_score': array([0.78995559, 0.84459427, 0.83561994]),\n",
       " 'split1_test_score': array([0.7998876 , 0.86527169, 0.85275664]),\n",
       " 'split2_test_score': array([0.77985087, 0.84676233, 0.83547477]),\n",
       " 'mean_test_score': array([0.78989935, 0.85220815, 0.84128306]),\n",
       " 'std_test_score': array([0.00817872, 0.00927873, 0.00811247]),\n",
       " 'rank_test_score': array([3, 1, 2]),\n",
       " 'split0_train_score': array([0.92164316, 0.99764088, 1.        ]),\n",
       " 'split1_train_score': array([0.91125937, 0.99862645, 1.        ]),\n",
       " 'split2_train_score': array([0.92065586, 0.99725127, 1.        ]),\n",
       " 'mean_train_score': array([0.9178528 , 0.99783953, 1.        ]),\n",
       " 'std_train_score': array([0.00467965, 0.00057872, 0.        ])}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRRRR still overfitting a ton. Let me drop the n_estimators again and see if that is my problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:    4.0s remaining:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   14.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=12, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [50, 100, 500, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'n_estimators':[50,100,500,1000]}\n",
    "clf = RandomForestClassifier(n_jobs = -1, max_depth = 12, class_weight = 'balanced')\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_trainn, y_trainn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.1513199 , 0.29151551, 1.7644697 , 5.74315246]),\n",
       " 'std_fit_time': array([0.00365055, 0.01203207, 0.46941196, 0.57235086]),\n",
       " 'mean_score_time': array([0.10703468, 0.14062405, 0.53889267, 0.75842341]),\n",
       " 'std_score_time': array([0.00047798, 0.04795485, 0.05857628, 0.19520868]),\n",
       " 'param_n_estimators': masked_array(data=[50, 100, 500, 1000],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_estimators': 50},\n",
       "  {'n_estimators': 100},\n",
       "  {'n_estimators': 500},\n",
       "  {'n_estimators': 1000}],\n",
       " 'split0_test_score': array([0.82113415, 0.82323382, 0.83153116, 0.8321013 ]),\n",
       " 'split1_test_score': array([0.83540334, 0.83950906, 0.84707442, 0.84581016]),\n",
       " 'split2_test_score': array([0.80620271, 0.81733772, 0.82311809, 0.8187843 ]),\n",
       " 'mean_test_score': array([0.82091538, 0.82669385, 0.83390868, 0.83223365]),\n",
       " 'std_test_score': array([0.01192016, 0.00937513, 0.00992213, 0.01103185]),\n",
       " 'rank_test_score': array([4, 3, 1, 2]),\n",
       " 'split0_train_score': array([0.96860879, 0.97522962, 0.9791642 , 0.9793557 ]),\n",
       " 'split1_train_score': array([0.96599363, 0.97336278, 0.97687701, 0.97765499]),\n",
       " 'split2_train_score': array([0.96648713, 0.9727334 , 0.97763176, 0.9766172 ]),\n",
       " 'mean_train_score': array([0.96702985, 0.97377527, 0.97789099, 0.97787596]),\n",
       " 'std_train_score': array([0.00113451, 0.00106   , 0.00095156, 0.00112885])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I'm not sure there is much modeling tweaks I can make to do much better than that without going into feature engineering. That's okay! I want to try a K-Nearest Neighbors model real quick and then I'll go come back to my random forest with some oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to try the same tactic by going one parameter at a time and seeing what works well with my data. I'll be doing it with my first training set of data that had about 20 features. I'm nervous to try it with all 133 features because of that evil and destructive curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    6.7s remaining:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   10.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'algorithm': ['ball_tree', 'kd_tree']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "param_dictionary = {'algorithm':['ball_tree','kd_tree']}\n",
    "clf = KNeighborsClassifier(n_neighbors = 3, )\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02672919, 0.02362641]),\n",
       " 'std_fit_time': array([0.01148236, 0.00700507]),\n",
       " 'mean_score_time': array([0.22094782, 0.0808181 ]),\n",
       " 'std_score_time': array([0.03491352, 0.02149258]),\n",
       " 'param_algorithm': masked_array(data=['ball_tree', 'kd_tree'],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'algorithm': 'ball_tree'}, {'algorithm': 'kd_tree'}],\n",
       " 'split0_test_score': array([0.59341911, 0.59443865]),\n",
       " 'split1_test_score': array([0.60078897, 0.59959049]),\n",
       " 'split2_test_score': array([0.59755385, 0.59739934]),\n",
       " 'mean_test_score': array([0.59725294, 0.59714209]),\n",
       " 'std_test_score': array([0.00301663, 0.00211135]),\n",
       " 'rank_test_score': array([1, 2]),\n",
       " 'split0_train_score': array([0.77059598, 0.7707506 ]),\n",
       " 'split1_train_score': array([0.77580873, 0.77625935]),\n",
       " 'split2_train_score': array([0.78147457, 0.78169698]),\n",
       " 'mean_train_score': array([0.77595976, 0.77623564]),\n",
       " 'std_train_score': array([0.00444245, 0.00446887])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well those are both equally awful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different numbers of neighbors that can 'vote'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:    6.9s remaining:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   14.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [1, 3, 5, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1_weighted',\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'n_neighbors':[1,3,5,10]}\n",
    "clf = KNeighborsClassifier(algorithm = 'ball_tree', )\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02808833, 0.02821302, 0.02848538, 0.02335207]),\n",
       " 'std_fit_time': array([0.0083323 , 0.01278608, 0.00591575, 0.0042196 ]),\n",
       " 'mean_score_time': array([0.21105822, 0.22197564, 0.33626405, 0.36661609]),\n",
       " 'std_score_time': array([0.03369901, 0.01410997, 0.06124148, 0.02017055]),\n",
       " 'param_n_neighbors': masked_array(data=[1, 3, 5, 10],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_neighbors': 1},\n",
       "  {'n_neighbors': 3},\n",
       "  {'n_neighbors': 5},\n",
       "  {'n_neighbors': 10}],\n",
       " 'split0_test_score': array([0.62686668, 0.59341911, 0.5837184 , 0.57902614]),\n",
       " 'split1_test_score': array([0.6322632 , 0.60078897, 0.60424188, 0.59208965]),\n",
       " 'split2_test_score': array([0.63011436, 0.59755385, 0.5963759 , 0.58437784]),\n",
       " 'mean_test_score': array([0.62974728, 0.59725294, 0.59477563, 0.58516304]),\n",
       " 'std_test_score': array([0.00221863, 0.00301663, 0.00845577, 0.00536265]),\n",
       " 'rank_test_score': array([1, 2, 3, 4]),\n",
       " 'split0_train_score': array([0.98589621, 0.77059598, 0.72707762, 0.64710445]),\n",
       " 'split1_train_score': array([0.98784552, 0.77580873, 0.71483761, 0.64087054]),\n",
       " 'split2_train_score': array([0.98781558, 0.78147457, 0.7318418 , 0.64775602]),\n",
       " 'mean_train_score': array([0.98718577, 0.77595976, 0.72458568, 0.64524367]),\n",
       " 'std_train_score': array([0.00091194, 0.00444245, 0.00716208, 0.00310369])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes..... They're all horrible Nelson! Overfitting on the low side and just plain BAD on the large side! Mayve weighting things differently will make our f1 better? I'll keep my n_neighbors at 10 to see how it effects a number that isn't overfitting a crazy ton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    6.0s remaining:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   10.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'weights':['uniform','distance']}\n",
    "clf = KNeighborsClassifier(algorithm = 'ball_tree', n_neighbors = 10 )\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02383931, 0.0223194 ]),\n",
       " 'std_fit_time': array([0.00579015, 0.00783267]),\n",
       " 'mean_score_time': array([0.28040973, 0.30153688]),\n",
       " 'std_score_time': array([0.02054649, 0.04091048]),\n",
       " 'param_weights': masked_array(data=['uniform', 'distance'],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'weights': 'uniform'}, {'weights': 'distance'}],\n",
       " 'split0_test_score': array([0.57902614, 0.65158109]),\n",
       " 'split1_test_score': array([0.59208965, 0.66650489]),\n",
       " 'split2_test_score': array([0.58437784, 0.66238232]),\n",
       " 'mean_test_score': array([0.58516304, 0.66015357]),\n",
       " 'std_test_score': array([0.00536265, 0.00629376]),\n",
       " 'rank_test_score': array([2, 1]),\n",
       " 'split0_train_score': array([0.64710445, 0.9874393 ]),\n",
       " 'split1_train_score': array([0.64087054, 0.9896071 ]),\n",
       " 'split2_train_score': array([0.64775602, 0.9890289 ]),\n",
       " 'mean_train_score': array([0.64524367, 0.98869177]),\n",
       " 'std_train_score': array([0.00310369, 0.00091654])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uhh.. Just as awful. Just for kicks and giggles lets run a K-Nearest Neighbors on my 133 feature set of data as well as my 10 feature set of data and see if they change anything. I'll throw on weights and neighbors I'll keep at 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    2.6s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='distance'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'algorithm': ['ball_tree', 'kd_tree']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'algorithm':['ball_tree','kd_tree']}\n",
    "clf = KNeighborsClassifier(n_neighbors = 10, weights = 'distance')\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00765379, 0.00663471]),\n",
       " 'std_fit_time': array([0.00169318, 0.00043564]),\n",
       " 'mean_score_time': array([0.05911271, 0.03491497]),\n",
       " 'std_score_time': array([0.00503735, 0.00081889]),\n",
       " 'param_algorithm': masked_array(data=['ball_tree', 'kd_tree'],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'algorithm': 'ball_tree'}, {'algorithm': 'kd_tree'}],\n",
       " 'split0_test_score': array([0.59147215, 0.59008772]),\n",
       " 'split1_test_score': array([0.60545918, 0.6050912 ]),\n",
       " 'split2_test_score': array([0.59873593, 0.59604229]),\n",
       " 'mean_test_score': array([0.59855388, 0.59707204]),\n",
       " 'std_test_score': array([0.00571238, 0.00616904]),\n",
       " 'rank_test_score': array([1, 2]),\n",
       " 'split0_train_score': array([0.88481656, 0.88481656]),\n",
       " 'split1_train_score': array([0.88533555, 0.88533555]),\n",
       " 'split2_train_score': array([0.88635645, 0.88635645]),\n",
       " 'mean_train_score': array([0.88550285, 0.88550285]),\n",
       " 'std_train_score': array([0.00063969, 0.00063969])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super simple data set = BLEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    3.4s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='distance'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'algorithm': ['ball_tree', 'kd_tree']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dictionary = {'algorithm':['ball_tree','kd_tree']}\n",
    "clf = KNeighborsClassifier(n_neighbors = 10, weights = 'distance')\n",
    "gs = GridSearchCV(clf, param_dictionary, n_jobs=-1, scoring = 'f1_weighted', verbose = 2)\n",
    "gs.fit(X_trainn, y_trainn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Noah\\Anaconda33\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.04986699, 0.04290764]),\n",
       " 'std_fit_time': array([0.01095596, 0.00847614]),\n",
       " 'mean_score_time': array([0.23271171, 0.19721293]),\n",
       " 'std_score_time': array([0.02187088, 0.03416337]),\n",
       " 'param_algorithm': masked_array(data=['ball_tree', 'kd_tree'],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'algorithm': 'ball_tree'}, {'algorithm': 'kd_tree'}],\n",
       " 'split0_test_score': array([0.53655832, 0.53655832]),\n",
       " 'split1_test_score': array([0.5472562, 0.5472562]),\n",
       " 'split2_test_score': array([0.5413636, 0.5413636]),\n",
       " 'mean_test_score': array([0.54172473, 0.54172473]),\n",
       " 'std_test_score': array([0.00437542, 0.00437542]),\n",
       " 'rank_test_score': array([1, 1]),\n",
       " 'split0_train_score': array([1., 1.]),\n",
       " 'split1_train_score': array([1., 1.]),\n",
       " 'split2_train_score': array([1., 1.]),\n",
       " 'mean_train_score': array([1., 1.]),\n",
       " 'std_train_score': array([0., 0.])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the overfitting champion of champions is K-Nearest Neighbors on a data set with 133 features. You, my friend, are a horrible model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind oversampling is to create synthetic data points in the categories that have way less data in order to make it more \"fair\" on your model. I obviously am going to be trying this with my random forest and NOT my K-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomOverSampler' object has no attribute 'fit_resample'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-48842f98a1e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_trainn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_trainn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_trainn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_trainn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomOverSampler' object has no attribute 'fit_resample'"
     ]
    }
   ],
   "source": [
    "random = RandomOverSampler(random_state=0)\n",
    "x_trainn, y_trainn = random.fit_resample(X_trainn, y_trainn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nelson I'm going to get level with you. My anaconda prompt just had a spazz attack when I tried to install imblearn onto my jupyter notebook. For some reason it read in at least half of it, but I can't figure out why I can't fit_resample. I went to their webiste ( https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler ) and it even says \"In all cases use fit_resample\" sooooo... I have no idea what to do! BUT I PROMISE THAT A RANDOM FOREST WITH OVER SAMPLING WOULD'VE BEEN super awesome and I think would've helped my model. I think with the problems I was having with getting my f1 score higher, more data couldn't have hurt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, although I don't have a perfectly tuned model, I think my final random forest models were actually pretty good. If I were to submit some predictions to Kaggle obviously I'd set some predictions and throw them into another CSV but I'm not going to do that because I don't need to! The rubric says that 5% of our grade is our result on Kaggle but I remember you specifically telling us that we didn't need to do that. \n",
    "\n",
    "As far as the Homework Questions go, I think most of them I answered between each run of my models. If I were a company I would use my model as a screening because I think it does as well as a middle school nurse does on her scoliosis screenings. I think it predicts pretty well whether or not the family is at risk for poverty or not, but lacks in differentiating between poverty groups. If I could get the over sampling to work then this would change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks Nelson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
